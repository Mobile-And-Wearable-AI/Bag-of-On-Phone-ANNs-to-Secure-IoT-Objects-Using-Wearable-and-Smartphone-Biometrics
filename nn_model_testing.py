# -*- coding: utf-8 -*-
"""NN_Model_testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Fbk3yz2CvkZ0YNZI-EITdlLz7UwxkXj
"""

#!pip install xlsxwriter
import tensorflow as tf
from tensorflow.keras import regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout,MaxPooling1D, Conv1D, MaxPooling2D, Conv2D, Flatten, BatchNormalization, Activation, LSTM, TimeDistributed
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow.keras as keras
leaky_relu = tf.nn.leaky_relu

'''
NN structure set up
Function to create model, required for KerasClassifier, We can uncomment out the needed model with the configuration of interest. We do not run a for loop on this in order to 
break up the run time to smaller controllable pieces 
'''
configuration = "CRNN_32-32-16"

'''
# CNN
def create_model(learn_rate =.001, activation="relu", init='glorot_uniform', kernel_regularizer="l1", dropout=0, input_shape= (None,61)): # (None,168) for gait, (None,21) for HR
    batch_size = 100
    # create model
    model = Sequential()
    model.add(BatchNormalization())
    model.add(Conv1D(280, 8, padding = "same", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer)) 
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(128, 8, padding = "same", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(64, 8, padding = "same", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))
    model.add(Flatten())
    #model.add(Dropout(dropout))
    model.add(Dense(1, kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))


    # Compile model
    optimizer = tf.keras.optimizers.Adam(lr = learn_rate)
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model
'''
'''
# RNN
def create_model(inputshape=(None,10)): # (None,168) for gait, (None,21) for HR, 229 for HRG, 189 for HRG, 61 for HRB
    # create model
    model = Sequential()
    model.add(BatchNormalization())
    model.add(keras.Input(shape=inputshape))
    model.add(LSTM(280, return_sequences=True))
    model.add(LSTM(128, return_sequences=True))  
    model.add(LSTM(64, return_sequences=True))
    model.add(Flatten())
    # model.add(Dropout(dropout))
    model.add(Dense(1, kernel_regularizer=regularizers.l2(0.01), activation="sigmoid"))

    # Compile model
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model
'''

#CRNN
def create_model(learn_rate =.001, activation="relu", init='glorot_uniform', kernel_regularizer="l2", dropout=0, input_shape= (None,40)):
    batch_size = 100
    # create model
    model = Sequential()
    model.add(BatchNormalization())
    model.add(Conv1D(32, 8, padding = "same", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Conv1D(32, 8, padding = "same", kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))
    model.add(TimeDistributed(Flatten()))
    model.add(LSTM(16, return_sequences=True))
    #model.add(LSTM(32, return_sequences=True))
    model.add(Flatten())
    model.add(Dense(1, kernel_initializer=init, activation=activation, kernel_regularizer=kernel_regularizer))


    # Compile model
    optimizer = tf.keras.optimizers.Adam(lr = learn_rate)
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model


import warnings

with warnings.catch_warnings():
    warnings.filterwarnings(action="ignore", category=FutureWarning)
    import tensorflow as tf
    from tensorflow import keras


def namestr(obj, namespace):
    return [name for name in namespace if namespace[name] is obj]


tf.compat.v1.enable_eager_execution()

'''
Data Pull
Here I pull the train test data from excel files.
Files include:
  H_10pData_withNoise.xlsx
  HB_10pData_withNoise.xlsx
  HG_10pData_withNoise.xlsx
  HGB_10pData_withNoise.xlsx
'''

File_Name = "data\HB_10pData_withNoise.xlsx"

df = pd.read_excel(File_Name, "Sheet1", header=0, usecols="B:NP")

print(File_Name)

people = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
sounds = [1, 2, 3, 4, 5, 6]
scores_df = pd.DataFrame(None, columns=["Target ID", "Sound", "Model", "TP", "FN", "FP", "TN", "best params"])
validation_df = pd.DataFrame(None, columns=["Target ID", "Sound", "Model", "means", "stds", "params"])

df = df.sample(frac=1)

best_model = None
best_model_performance = 0
best_model_summary = None

for column in df.columns:
    if ((column != "Person ID") & (column != "Sound ID")):
        df[column] = df[column] / df[column].max()

pd.options.mode.use_inf_as_na = True
df = df.replace([np.inf, -np.inf], np.nan)
non_null_column = df.isnull().sum()[df.isnull().sum() == 0].index
df = df[non_null_column]

for person in people:
    validUser = person

    for sound in sounds:
        print("validUser is " + str(validUser))
        print("sound left out" + str(sound))
        traindf = df.loc[(df["Person ID"] == validUser) & (df["Sound ID"] != sound)]

        for person in people:
            if (person != validUser):
                traindf = pd.concat(
                    [traindf, df.loc[(df["Person ID"] == person) & (df["Sound ID"] != sound)][:len(
                        df.loc[(df["Person ID"] == validUser) & (df["Sound ID"] != sound)]) // (len(
                        people) - 1)]])

        testdf = df.loc[(df["Person ID"] == validUser) & (df["Sound ID"] == sound)]

        for person in people:
            if (person != validUser):
                testdf = pd.concat(
                    [testdf, df.loc[(df["Person ID"] == person) & (df["Sound ID"] == sound)][:len(
                        df.loc[(df["Person ID"] == validUser) & (df["Sound ID"] == sound)]) // (len(
                        people) - 1)]])

        traindf = traindf.sample(frac=1)
        testdf = testdf.sample(frac=1)

        X_train = traindf.drop(columns=["Person ID", "Sound ID"])
        X_test = testdf.drop(columns=["Person ID", "Sound ID"])

        Y_train = traindf["Person ID"]
        Y_test = testdf["Person ID"]
        for person in people:
            if (person != validUser):
                # print(person)
                Y_train = Y_train.replace({person: 0})
                Y_test = Y_test.replace({person: 0})

        for person in people:
            if (person == validUser):
                # print(person)
                Y_train = Y_train.replace({person: 1})
                Y_test = Y_test.replace({person: 1})

        print("Training Size: ", len(X_train), " balance of: ", Y_train.sum(), ":",
              len(Y_train) - Y_train.sum(), \
              "Training Size: ", len(X_test), " balance of : ", Y_test.sum(), ":", len(Y_test) - Y_test.sum())

        sc = StandardScaler()
        X_train = sc.fit_transform(X_train)
        X_test = sc.fit_transform(X_test)
        print("shapes", X_train.shape, Y_train.shape)
        X_train = X_train.reshape(-1, X_train.shape[1], 1)
        X_test = X_test.reshape(-1, X_train.shape[1], 1)

        print("final", X_train.shape, Y_train.shape)

        model = KerasClassifier(build_fn=create_model, verbose=0)
        epochs = [50,100]
        batches = [50,100] 
        param_grid = dict(epochs=epochs, batch_size=batches)  
        grid = GridSearchCV(estimator=model, param_grid=param_grid)
        grid_result = grid.fit(X_train, Y_train)
        means = grid_result.cv_results_['mean_test_score']
        stds = grid_result.cv_results_['std_test_score']
        params = grid_result.cv_results_['params']
        for mean, stdev, param in zip(means, stds, params):
            print("Trainning " + str(validUser) + " " + str(sound) + " %f (%f) with: %r" % (mean, stdev, param))
            validation_info = [validUser, sound, "CCRNN", means, stds, params]
            validation_df = validation_df.append(pd.Series(validation_info, index=validation_df.columns),
                                                  ignore_index=True)

        pred_keras = grid_result.predict(X_test)
        matrix = confusion_matrix(Y_test, pred_keras)
        TP, FN, FP, TN = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]
        score_info = [validUser, sound, "CCRNN", TP, FN, FP, TN, grid_result.best_params_]
        scores_df = scores_df.append(pd.Series(score_info, index=scores_df.columns), ignore_index=True)
        print("Testing " + str(validUser) + " " + str(sound) + " " + str(TP) + " " + str(FN) + " " + str(
            FP) + " " + str(TN))
        print("Testing " + str(validUser) + " " + str(sound) + " ", grid_result.best_params_)

        ''' To save trained model
        model_to_save = grid_result
        #best_model.save_model()
        converter = tf.lite.TFLiteConverter.from_keras_model(model_to_save)
        tflite_model = converter.convert()
        with tf.io.gfile.GFile("model.tflite", "wb") as f:
            f.write(tflite_model)
        '''

writer = pd.ExcelWriter("Kera_Results_for_" + configuration + ".xlsx", engine='xlsxwriter')
scores_df.to_excel(writer, sheet_name='Sheet1')
validation_df.to_excel(writer, sheet_name="Sheet2")
writer.save()
print(scores_df)